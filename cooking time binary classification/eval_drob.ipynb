{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    TextClassificationPipeline,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler((0, 1),clip=True)\n",
    "scaler.fit([[0], [100]])\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "model_name = 'distilroberta-base'\n",
    "# model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "MAX_LEN = 400\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 3e-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"test_drob_fsb_nolora.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./drob-fsb-nolora-output/checkpoint-50000\"\n",
    "model_c = transformers.AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2, load_in_8bit = True)\n",
    "model_c.config.problem_type = \"multi_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165023</th>\n",
       "      <td>Open the rolls and fill with the chicken. Mix ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149298</th>\n",
       "      <td>In wok or medium saucepan and over high flame,...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104703</th>\n",
       "      <td>Place in frying pan with 2 tablespoons butter ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140472</th>\n",
       "      <td>Preheat oven to 375 degrees. Grease cookie she...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185042</th>\n",
       "      <td>Preheat oven to 350 degrees. In a 9 x 13-inch ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153523</th>\n",
       "      <td>Mix all ingredients except cheese. Pour a laye...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386467</th>\n",
       "      <td>Sear roast on both sides in heavy roasting pan...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295885</th>\n",
       "      <td>Cream butter and sugar. Add well beaten eggs. ...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161548</th>\n",
       "      <td>Place all ingredients in a non-aluminum saucep...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335852</th>\n",
       "      <td>Saute onions in salad oil until tender, but no...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentences  target\n",
       "165023  Open the rolls and fill with the chicken. Mix ...  [0, 1]\n",
       "149298  In wok or medium saucepan and over high flame,...  [0, 1]\n",
       "104703  Place in frying pan with 2 tablespoons butter ...  [0, 1]\n",
       "140472  Preheat oven to 375 degrees. Grease cookie she...  [0, 1]\n",
       "185042  Preheat oven to 350 degrees. In a 9 x 13-inch ...  [0, 1]\n",
       "...                                                   ...     ...\n",
       "153523  Mix all ingredients except cheese. Pour a laye...  [0, 1]\n",
       "386467  Sear roast on both sides in heavy roasting pan...  [1, 0]\n",
       "295885  Cream butter and sugar. Add well beaten eggs. ...  [1, 0]\n",
       "161548  Place all ingredients in a non-aluminum saucep...  [0, 1]\n",
       "335852  Saute onions in salad oil until tender, but no...  [1, 0]\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class logitpipeline(TextClassificationPipeline):\n",
    "#     def postprocess(self, model_outputs):\n",
    "#         best_class = model_outputs[\"logits\"]\n",
    "#         return best_class\n",
    "pipe = TextClassificationPipeline(model=model_c, tokenizer=tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9998944997787476}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Cook for 15 minutes until softened.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = data.sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "count = 0\n",
    "def dataset():\n",
    "    for i in range(len(sents)):\n",
    "        yield sents[i]\n",
    "        \n",
    "for out in pipe(dataset()):\n",
    "    count +=1\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "    output.append(out)\n",
    "# for s in sents:\n",
    "#     count +=1\n",
    "#     if count%100==0:\n",
    "#         print(count)\n",
    "#     output.append(pipe(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame({\"output\":[i for i in output]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_parquet(\"pred_drob_nolora.gzip\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9094254970550537},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9998706579208374},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9997730851173401},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9997507929801941},\n",
       " {'label': 'LABEL_0', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.9998804330825806},\n",
       " {'label': 'LABEL_0', 'score': 0.9998313188552856},\n",
       " {'label': 'LABEL_0', 'score': 0.99988853931427},\n",
       " {'label': 'LABEL_0', 'score': 0.9998390674591064},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9994407296180725},\n",
       " {'label': 'LABEL_0', 'score': 0.9998273253440857},\n",
       " {'label': 'LABEL_0', 'score': 0.9492946267127991},\n",
       " {'label': 'LABEL_0', 'score': 0.9780936241149902},\n",
       " {'label': 'LABEL_0', 'score': 0.9999016523361206},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9175984263420105},\n",
       " {'label': 'LABEL_0', 'score': 0.9734938144683838},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.6771538853645325},\n",
       " {'label': 'LABEL_0', 'score': 0.998504638671875},\n",
       " {'label': 'LABEL_1', 'score': 0.9998300075531006},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.99988853931427},\n",
       " {'label': 'LABEL_0', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999083280563354},\n",
       " {'label': 'LABEL_0', 'score': 0.9998928308486938},\n",
       " {'label': 'LABEL_0', 'score': 0.9640516638755798},\n",
       " {'label': 'LABEL_0', 'score': 0.63238924741745},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9998612403869629},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999001026153564},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9488224983215332},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9998326301574707},\n",
       " {'label': 'LABEL_0', 'score': 0.9998557567596436},\n",
       " {'label': 'LABEL_0', 'score': 0.9805806875228882},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_0', 'score': 0.999891996383667},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.999868631362915},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_0', 'score': 0.9997783303260803},\n",
       " {'label': 'LABEL_0', 'score': 0.9997918009757996},\n",
       " {'label': 'LABEL_0', 'score': 0.9991990923881531},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9997783303260803},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.999919056892395},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9996874332427979},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.999891996383667},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_0', 'score': 0.9998326301574707},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_0', 'score': 0.937210738658905},\n",
       " {'label': 'LABEL_0', 'score': 0.9998013377189636},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.898944079875946},\n",
       " {'label': 'LABEL_0', 'score': 0.9999384880065918},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9751545190811157},\n",
       " {'label': 'LABEL_0', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9998499155044556},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_0', 'score': 0.9995660185813904},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.999508261680603},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9998557567596436},\n",
       " {'label': 'LABEL_0', 'score': 0.9998557567596436},\n",
       " {'label': 'LABEL_0', 'score': 0.9969245791435242},\n",
       " {'label': 'LABEL_0', 'score': 0.9997658133506775},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_1', 'score': 0.9999285936355591},\n",
       " {'label': 'LABEL_0', 'score': 0.9996033310890198},\n",
       " {'label': 'LABEL_0', 'score': 0.8416955471038818},\n",
       " {'label': 'LABEL_0', 'score': 0.9995469450950623},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9999245405197144},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_0', 'score': 0.9996646642684937},\n",
       " {'label': 'LABEL_0', 'score': 0.9894295334815979},\n",
       " {'label': 'LABEL_0', 'score': 0.9999117851257324},\n",
       " {'label': 'LABEL_0', 'score': 0.9997584223747253},\n",
       " {'label': 'LABEL_0', 'score': 0.9999103546142578},\n",
       " {'label': 'LABEL_0', 'score': 0.999891996383667},\n",
       " {'label': 'LABEL_0', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9425067901611328},\n",
       " {'label': 'LABEL_0', 'score': 0.851212203502655},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9110215306282043},\n",
       " {'label': 'LABEL_0', 'score': 0.9998601675033569},\n",
       " {'label': 'LABEL_0', 'score': 0.9999068975448608},\n",
       " {'label': 'LABEL_0', 'score': 0.9272316098213196},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9998840093612671},\n",
       " {'label': 'LABEL_0', 'score': 0.9997546076774597},\n",
       " {'label': 'LABEL_0', 'score': 0.8901033401489258},\n",
       " {'label': 'LABEL_0', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9998804330825806},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.999783456325531},\n",
       " {'label': 'LABEL_1', 'score': 0.9999165534973145},\n",
       " {'label': 'LABEL_0', 'score': 0.9998452663421631},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.8562121987342834},\n",
       " {'label': 'LABEL_0', 'score': 0.9999151229858398},\n",
       " {'label': 'LABEL_0', 'score': 0.9799382090568542},\n",
       " {'label': 'LABEL_0', 'score': 0.731634259223938},\n",
       " {'label': 'LABEL_0', 'score': 0.9504110217094421},\n",
       " {'label': 'LABEL_1', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_0', 'score': 0.9897116422653198},\n",
       " {'label': 'LABEL_0', 'score': 0.9520419239997864},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.999869704246521},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9993307590484619},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_0', 'score': 0.9440738558769226},\n",
       " {'label': 'LABEL_0', 'score': 0.9999279975891113},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9985449314117432},\n",
       " {'label': 'LABEL_0', 'score': 0.9999196529388428},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9985730648040771},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.999906063079834},\n",
       " {'label': 'LABEL_0', 'score': 0.9998612403869629},\n",
       " {'label': 'LABEL_0', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9814888834953308},\n",
       " {'label': 'LABEL_0', 'score': 0.8802834153175354},\n",
       " {'label': 'LABEL_0', 'score': 0.9997325539588928},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999291896820068},\n",
       " {'label': 'LABEL_0', 'score': 0.9998365640640259},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.91804039478302},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9692078232765198},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.8840392827987671},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9388007521629333},\n",
       " {'label': 'LABEL_0', 'score': 0.9994557499885559},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9135223627090454},\n",
       " {'label': 'LABEL_0', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.999841570854187},\n",
       " {'label': 'LABEL_0', 'score': 0.9724147915840149},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9532750844955444},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9998928308486938},\n",
       " {'label': 'LABEL_0', 'score': 0.9993487000465393},\n",
       " {'label': 'LABEL_0', 'score': 0.9999151229858398},\n",
       " {'label': 'LABEL_0', 'score': 0.9873262047767639},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9996287822723389},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9999030828475952},\n",
       " {'label': 'LABEL_0', 'score': 0.9999068975448608},\n",
       " {'label': 'LABEL_0', 'score': 0.9999053478240967},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.999906063079834},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9998717308044434},\n",
       " {'label': 'LABEL_0', 'score': 0.9003546833992004},\n",
       " {'label': 'LABEL_0', 'score': 0.9998984336853027},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999151229858398},\n",
       " {'label': 'LABEL_0', 'score': 0.9998218417167664},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9997867941856384},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9585376977920532},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9998557567596436},\n",
       " {'label': 'LABEL_0', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_0', 'score': 0.9580694437026978},\n",
       " {'label': 'LABEL_0', 'score': 0.9936116337776184},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9992145299911499},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9991672039031982},\n",
       " {'label': 'LABEL_0', 'score': 0.9997918009757996},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.999762237071991},\n",
       " {'label': 'LABEL_0', 'score': 0.9997712969779968},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9998013377189636},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9998984336853027},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9939496517181396},\n",
       " {'label': 'LABEL_0', 'score': 0.9442797303199768},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9986114501953125},\n",
       " {'label': 'LABEL_0', 'score': 0.8237541317939758},\n",
       " {'label': 'LABEL_1', 'score': 0.9983385801315308},\n",
       " {'label': 'LABEL_0', 'score': 0.9559813141822815},\n",
       " {'label': 'LABEL_1', 'score': 0.9999291896820068},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9999196529388428},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9407896995544434},\n",
       " {'label': 'LABEL_0', 'score': 0.999464213848114},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9998992681503296},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9990072846412659},\n",
       " {'label': 'LABEL_0', 'score': 0.997730553150177},\n",
       " {'label': 'LABEL_0', 'score': 0.9996607303619385},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_0', 'score': 0.9998286962509155},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9998633861541748},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.9999076128005981},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9799765348434448},\n",
       " {'label': 'LABEL_0', 'score': 0.9415467381477356},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_0', 'score': 0.9130584001541138},\n",
       " {'label': 'LABEL_0', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9998534917831421},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9998775720596313},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999384880065918},\n",
       " {'label': 'LABEL_0', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.969613254070282},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9998148083686829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9997387528419495},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.9999090433120728},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9954615235328674},\n",
       " {'label': 'LABEL_0', 'score': 0.9998928308486938},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.999891996383667},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_1', 'score': 0.999929666519165},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9992114305496216},\n",
       " {'label': 'LABEL_0', 'score': 0.9997901320457458},\n",
       " {'label': 'LABEL_1', 'score': 0.7563575506210327},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_0', 'score': 0.999919056892395},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9997695088386536},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9979169964790344},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.999868631362915},\n",
       " {'label': 'LABEL_0', 'score': 0.9997367262840271},\n",
       " {'label': 'LABEL_0', 'score': 0.9998893737792969},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9998936653137207},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_0', 'score': 0.9996672868728638},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.999913215637207},\n",
       " {'label': 'LABEL_0', 'score': 0.9511421918869019},\n",
       " {'label': 'LABEL_0', 'score': 0.9337939620018005},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.8267117738723755},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.5412876009941101},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9921537041664124},\n",
       " {'label': 'LABEL_0', 'score': 0.9960851669311523},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9996874332427979},\n",
       " {'label': 'LABEL_0', 'score': 0.999913215637207},\n",
       " {'label': 'LABEL_0', 'score': 0.9997817873954773},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9999046325683594},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9998745918273926},\n",
       " {'label': 'LABEL_0', 'score': 0.7958168387413025},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_0', 'score': 0.8848377466201782},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9999046325683594},\n",
       " {'label': 'LABEL_0', 'score': 0.99991774559021},\n",
       " {'label': 'LABEL_1', 'score': 0.9998286962509155},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.925499677658081},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_0', 'score': 0.9999001026153564},\n",
       " {'label': 'LABEL_0', 'score': 0.9878996014595032},\n",
       " {'label': 'LABEL_0', 'score': 0.9998717308044434},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9962051510810852},\n",
       " {'label': 'LABEL_0', 'score': 0.9998794794082642},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9998273253440857},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.999859094619751},\n",
       " {'label': 'LABEL_0', 'score': 0.9998133778572083},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9996373653411865},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9999384880065918},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9091032147407532},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9998893737792969},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_0', 'score': 0.999841570854187},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9998775720596313},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.999891996383667},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999158382415771},\n",
       " {'label': 'LABEL_0', 'score': 0.9813465476036072},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9664738178253174},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9381240010261536},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_0', 'score': 0.999841570854187},\n",
       " {'label': 'LABEL_0', 'score': 0.9998612403869629},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9526622891426086},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9992175102233887},\n",
       " {'label': 'LABEL_0', 'score': 0.9998960494995117},\n",
       " {'label': 'LABEL_0', 'score': 0.9999165534973145},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.9998726844787598},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9144436717033386},\n",
       " {'label': 'LABEL_0', 'score': 0.9998044371604919},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9998736381530762},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_0', 'score': 0.997188150882721},\n",
       " {'label': 'LABEL_0', 'score': 0.9999158382415771},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.99988853931427},\n",
       " {'label': 'LABEL_0', 'score': 0.9592307209968567},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9992591738700867},\n",
       " {'label': 'LABEL_0', 'score': 0.9998867511749268},\n",
       " {'label': 'LABEL_0', 'score': 0.9998568296432495},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9998717308044434},\n",
       " {'label': 'LABEL_0', 'score': 0.9994578957557678},\n",
       " {'label': 'LABEL_0', 'score': 0.9648551344871521},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9999053478240967},\n",
       " {'label': 'LABEL_1', 'score': 0.9996402263641357},\n",
       " {'label': 'LABEL_0', 'score': 0.9998875856399536},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_0', 'score': 0.9998875856399536},\n",
       " {'label': 'LABEL_0', 'score': 0.9997387528419495},\n",
       " {'label': 'LABEL_0', 'score': 0.9758085012435913},\n",
       " {'label': 'LABEL_1', 'score': 0.9999239444732666},\n",
       " {'label': 'LABEL_0', 'score': 0.9999274015426636},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9998867511749268},\n",
       " {'label': 'LABEL_0', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_0', 'score': 0.9509603977203369},\n",
       " {'label': 'LABEL_0', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9998944997787476},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_0', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9998717308044434},\n",
       " {'label': 'LABEL_0', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.999929666519165},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.999929666519165},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9272316098213196},\n",
       " {'label': 'LABEL_0', 'score': 0.9997730851173401},\n",
       " {'label': 'LABEL_0', 'score': 0.9998579025268555},\n",
       " {'label': 'LABEL_0', 'score': 0.7463239431381226},\n",
       " {'label': 'LABEL_0', 'score': 0.9990745782852173},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9457011222839355},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_0', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.999045193195343},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9998013377189636},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9998675584793091},\n",
       " {'label': 'LABEL_0', 'score': 0.9999076128005981},\n",
       " {'label': 'LABEL_0', 'score': 0.9998218417167664},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9829546809196472},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9998655319213867},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_0', 'score': 0.999823272228241},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9999184608459473},\n",
       " {'label': 'LABEL_0', 'score': 0.9997933506965637},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_0', 'score': 0.9999171495437622},\n",
       " {'label': 'LABEL_0', 'score': 0.8610715270042419},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.954562783241272},\n",
       " {'label': 'LABEL_0', 'score': 0.9998655319213867},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9267027974128723},\n",
       " {'label': 'LABEL_1', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_0', 'score': 0.9998910427093506},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9998893737792969},\n",
       " {'label': 'LABEL_0', 'score': 0.9998247027397156},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9988840222358704},\n",
       " {'label': 'LABEL_0', 'score': 0.9999083280563354},\n",
       " {'label': 'LABEL_0', 'score': 0.9998133778572083},\n",
       " {'label': 'LABEL_0', 'score': 0.9999221563339233},\n",
       " {'label': 'LABEL_0', 'score': 0.9998849630355835},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9998910427093506},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9997918009757996},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9994705319404602},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.9227612018585205},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9998726844787598},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.9996699094772339},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9885804057121277},\n",
       " {'label': 'LABEL_0', 'score': 0.9999384880065918},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9999384880065918},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9998176693916321},\n",
       " {'label': 'LABEL_0', 'score': 0.945600688457489},\n",
       " {'label': 'LABEL_0', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_0', 'score': 0.9998726844787598},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9792749285697937},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.999180018901825},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_0', 'score': 0.9997565150260925},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9998273253440857},\n",
       " {'label': 'LABEL_0', 'score': 0.9999233484268188},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9998893737792969},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.999906063079834},\n",
       " {'label': 'LABEL_0', 'score': 0.9105454087257385},\n",
       " {'label': 'LABEL_0', 'score': 0.9503188729286194},\n",
       " {'label': 'LABEL_1', 'score': 0.9999285936355591},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_0', 'score': 0.999220609664917},\n",
       " {'label': 'LABEL_1', 'score': 0.991355836391449},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9413313865661621},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_0', 'score': 0.9974689483642578},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9997884631156921},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9998859167098999},\n",
       " {'label': 'LABEL_0', 'score': 0.8982321619987488},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9998511075973511},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9961903095245361},\n",
       " {'label': 'LABEL_1', 'score': 0.9999291896820068},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9998840093612671},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9995216131210327},\n",
       " {'label': 'LABEL_0', 'score': 0.9998706579208374},\n",
       " {'label': 'LABEL_0', 'score': 0.9966495633125305},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9998218417167664},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_0', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_0', 'score': 0.9998655319213867},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9993040561676025},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9359345436096191},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9999083280563354},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9998601675033569},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_1', 'score': 0.9997040629386902},\n",
       " {'label': 'LABEL_0', 'score': 0.999913215637207},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9205042719841003},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.907967209815979},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9998476505279541},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.5868780016899109},\n",
       " {'label': 'LABEL_0', 'score': 0.9998326301574707},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.999854564666748},\n",
       " {'label': 'LABEL_0', 'score': 0.9998300075531006},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9998717308044434},\n",
       " {'label': 'LABEL_0', 'score': 0.9999227523803711},\n",
       " {'label': 'LABEL_0', 'score': 0.8946054577827454},\n",
       " {'label': 'LABEL_0', 'score': 0.9999103546142578},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9997817873954773},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999291896820068},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9904775619506836},\n",
       " {'label': 'LABEL_0', 'score': 0.9998977184295654},\n",
       " {'label': 'LABEL_0', 'score': 0.9501341581344604},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9997040629386902},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9993613362312317},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9998952150344849},\n",
       " {'label': 'LABEL_0', 'score': 0.9998476505279541},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9997901320457458},\n",
       " {'label': 'LABEL_0', 'score': 0.937325656414032},\n",
       " {'label': 'LABEL_0', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9996033310890198},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9999083280563354},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9596865773200989},\n",
       " {'label': 'LABEL_0', 'score': 0.9334307909011841},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.999929666519165},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9998893737792969},\n",
       " {'label': 'LABEL_0', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9997527003288269},\n",
       " {'label': 'LABEL_0', 'score': 0.9998960494995117},\n",
       " {'label': 'LABEL_1', 'score': 0.9999291896820068},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9492005109786987},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9998849630355835},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9382373094558716},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9731897711753845},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9749168753623962},\n",
       " {'label': 'LABEL_0', 'score': 0.9129031896591187},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9998511075973511},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_0', 'score': 0.9399133324623108},\n",
       " {'label': 'LABEL_0', 'score': 0.9998717308044434},\n",
       " {'label': 'LABEL_0', 'score': 0.9763556718826294},\n",
       " {'label': 'LABEL_0', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_0', 'score': 0.9999221563339233},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9998601675033569},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.9539660811424255},\n",
       " {'label': 'LABEL_0', 'score': 0.9997428059577942},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9999209642410278},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9990818500518799},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9998205304145813},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9998785257339478},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9998666048049927},\n",
       " {'label': 'LABEL_0', 'score': 0.9987840056419373},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9998568296432495},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9998992681503296},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999285936355591},\n",
       " {'label': 'LABEL_1', 'score': 0.9999279975891113},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.9911527037620544},\n",
       " {'label': 'LABEL_0', 'score': 0.9274948239326477},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_0', 'score': 0.9999053478240967},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9998644590377808},\n",
       " {'label': 'LABEL_0', 'score': 0.9998968839645386},\n",
       " {'label': 'LABEL_0', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_0', 'score': 0.99993896484375},\n",
       " {'label': 'LABEL_0', 'score': 0.9998717308044434},\n",
       " {'label': 'LABEL_0', 'score': 0.9999369382858276},\n",
       " {'label': 'LABEL_0', 'score': 0.9998910427093506},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.999840259552002},\n",
       " {'label': 'LABEL_0', 'score': 0.9910838603973389},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999068975448608},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9998756647109985},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999339580535889},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9999138116836548},\n",
       " {'label': 'LABEL_0', 'score': 0.9999384880065918},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9492946267127991},\n",
       " {'label': 'LABEL_1', 'score': 0.9999324083328247},\n",
       " {'label': 'LABEL_0', 'score': 0.9998260140419006},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999313354492188},\n",
       " {'label': 'LABEL_1', 'score': 0.999935507774353},\n",
       " {'label': 'LABEL_0', 'score': 0.9992591738700867},\n",
       " {'label': 'LABEL_0', 'score': 0.9999151229858398},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_0', 'score': 0.9999030828475952},\n",
       " {'label': 'LABEL_1', 'score': 0.9999334812164307},\n",
       " {'label': 'LABEL_0', 'score': 0.9998148083686829},\n",
       " {'label': 'LABEL_1', 'score': 0.999930739402771},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9393594861030579},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_1', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_0', 'score': 0.9999374151229858},\n",
       " {'label': 'LABEL_1', 'score': 0.9999302625656128},\n",
       " {'label': 'LABEL_1', 'score': 0.9999364614486694},\n",
       " {'label': 'LABEL_0', 'score': 0.9974090456962585},\n",
       " {'label': 'LABEL_0', 'score': 0.8345417976379395},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " {'label': 'LABEL_0', 'score': 0.9998893737792969},\n",
       " {'label': 'LABEL_0', 'score': 0.9997584223747253},\n",
       " {'label': 'LABEL_0', 'score': 0.999869704246521},\n",
       " {'label': 'LABEL_1', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_1', 'score': 0.9999291896820068},\n",
       " {'label': 'LABEL_1', 'score': 0.9999349117279053},\n",
       " {'label': 'LABEL_0', 'score': 0.9999344348907471},\n",
       " {'label': 'LABEL_1', 'score': 0.9999359846115112},\n",
       " {'label': 'LABEL_0', 'score': 0.9998936653137207},\n",
       " {'label': 'LABEL_0', 'score': 0.9452984929084778},\n",
       " {'label': 'LABEL_0', 'score': 0.9544779658317566},\n",
       " {'label': 'LABEL_0', 'score': 0.9998984336853027},\n",
       " {'label': 'LABEL_0', 'score': 0.9997565150260925},\n",
       " {'label': 'LABEL_0', 'score': 0.999937891960144},\n",
       " {'label': 'LABEL_0', 'score': 0.9504110217094421},\n",
       " {'label': 'LABEL_1', 'score': 0.999931812286377},\n",
       " {'label': 'LABEL_1', 'score': 0.9999328851699829},\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings = tokenizer.batch_encode_plus(\n",
    "#                     data['sentences'].tolist(),\n",
    "#                     add_special_tokens=True,\n",
    "#                     max_length=MAX_LEN,\n",
    "#                     padding='max_length',\n",
    "#                     return_token_type_ids=True,\n",
    "#                     truncation=True,\n",
    "#                     return_tensors=\"pt\"\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# def enc(sent)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m encodings \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m      3\u001b[0m                     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m      4\u001b[0m                     add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m                     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m                 )\n\u001b[1;32m---> 11\u001b[0m ret \u001b[38;5;241m=\u001b[39m [val\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m encodings\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[0;32m     12\u001b[0m ret\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# def enc(sent)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m encodings \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m      3\u001b[0m                     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m      4\u001b[0m                     add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m                     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m                 )\n\u001b[1;32m---> 11\u001b[0m ret \u001b[38;5;241m=\u001b[39m [\u001b[43mval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m encodings\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[0;32m     12\u001b[0m ret\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# # def enc(sent)\n",
    "# encodings = tokenizer.encode_plus(\n",
    "#                     data['sentences'].tolist()[0],\n",
    "#                     add_special_tokens=True,\n",
    "#                     max_length=MAX_LEN,\n",
    "#                     padding='max_length',\n",
    "#                     return_token_type_ids=True,\n",
    "#                     truncation=True,\n",
    "#                     return_tensors=\"pt\"\n",
    "#                 )\n",
    "# ret = [val.clone().detach().to(\"cuda\") for val in encodings.values()]\n",
    "# ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class cooking_time_model(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(cooking_time_model, self).__init__()\n",
    "#         self.l1 = transformers.RobertaModel.from_pretrained(pretrained)\n",
    "#         self.l2 = torch.nn.Dropout(0.3)\n",
    "#         self.l3 = torch.nn.Linear(768, MAX_LEN)\n",
    "\n",
    "#     def forward(self, ids, mask, token_type_ids):\n",
    "#         output_1 = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "#         output_2 = self.l2(output_1.pooler_output)\n",
    "#         output = self.l3(output_2)\n",
    "#         return output\n",
    "model_path = \"./drob-mfst-nolora-output/checkpoint-17500\"\n",
    "model_c = transformers.AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=1, load_in_8bit = True)\n",
    "model_c.config.problem_type = \"regression\"\n",
    "# model_c = cooking_time_model()\n",
    "# model_c.to(device)\n",
    "# def count_parameters(modelr):\n",
    "#     return sum(p.numel() for p in modelr.parameters() if p.requires_grad)\n",
    "\n",
    "# print(f'The model has {count_parameters(model_c):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_c\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 3\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mret\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mret\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mret\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1198\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1210\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:828\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    836\u001b[0m     embedding_output,\n\u001b[0;32m    837\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    845\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    846\u001b[0m )\n\u001b[0;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:132\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    130\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m    131\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m--> 132\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2542\u001b[0m     )\n\u001b[1;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model_c.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_c(input_ids=ret[0],attention_mask=ret[1],token_type_ids=ret[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into\n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss\n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.00022455735384061483 0.00022455735384061483\n"
     ]
    }
   ],
   "source": [
    "model_best, _, _, l1 = load_ckp(\"best_model_a3.pt\", model_c, optimizer)\n",
    "model_current, _, _, l2 = load_ckp(\"current_checkpoint_a3.pt\", model_c, optimizer)\n",
    "print(l1==l2)\n",
    "print(l1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~ 69950/69998(0.9993142661218892) ~~~~~~~~~~~~~~~~\r"
     ]
    }
   ],
   "source": [
    "def submission(dataloader, model_selected): # Code for submission\n",
    "    preds = []\n",
    "    targets = []\n",
    "    model_selected.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (id, mask, tokentype, target) in enumerate(dataloader):\n",
    "            outputs = model_selected(id, mask, tokentype)\n",
    "            preds.append(list(outputs.logits))\n",
    "            targets.append(list(target))\n",
    "            if (batch_idx+1)%50 == 0:\n",
    "                print(\"~\"*15,f\"{batch_idx+1}/{len(dataloader)}({(batch_idx+1)/len(dataloader)})\",\"~\"*15, end=\"\\r\")\n",
    "            \n",
    "    return preds, targets\n",
    "ret, tar = submission(test_loader, model_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltar = [l[0].tolist() for l in tar]\n",
    "lret = [l[0].tolist()[0] for l in ret]\n",
    "ltar = [round(l, 4) for l in ltar]\n",
    "lret = [round(l, 4) if l>=0 else 0 for l in lret]\n",
    "# lret = [l[0] for l in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006316959055973027\n"
     ]
    }
   ],
   "source": [
    "# for p, r in zip(ltar, lret):\n",
    "#     # print(p, r)\n",
    "#     print(abs(p-r)*100)\n",
    "err = [abs(p-r) for p,r in zip(ltar,lret)]\n",
    "print(np.mean(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error stats:\n",
      "average: 0.6316959055973027\n",
      "max: 92.7\n",
      "max error target(minutes) 93.5\n",
      "max error prediction(minutes) 0.8\n",
      "max error recipe:\n",
      "1 elephant, 2 rabbits (optional), 2 pails salt, 1 (8 oz.) canister black pepper, 3 bushels onions, <mask> gal. water, 6 pails flour\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler((0, 1),clip=True)\n",
    "scaler.fit([[0], [100]])\n",
    "err_min = scaler.inverse_transform([[i] for i in err])\n",
    "# for m, e in zip(err_min, err):\n",
    "#     print(m[0], e)\n",
    "print(\"error stats:\")\n",
    "print(\"average:\", np.mean(err_min))\n",
    "print(\"max:\", *err_min[np.argmax(err_min)])\n",
    "print(\"max error target(minutes)\",(scaler.inverse_transform([[data.iloc[np.argmax(err_min)][1]]])[0][0]))\n",
    "print(\"max error prediction(minutes)\",scaler.inverse_transform([[lret[np.argmax(err_min)]]])[0][0])\n",
    "print(\"max error recipe:\")\n",
    "print(data.iloc[np.argmax(err_min)][0])\n",
    "# data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08980672806501389\n",
      "8.980672806501389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.12]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_input = [\"<mask> lb. lean beef, cut across the grain into thin slices (you need a quick-cooking cut, such as minute steak), 1 lemongrass stalk, trimmed and finely chopped, <mask> tbsp. soy sauce, 2 tbs. fish sauce, 4 tsp. brown sugar, 0.5 tsp. chilli flakes, 1 lime, juiced, 3 tbsp. vegetable oil, 1 green pepper, thinly sliced, 2 bunches of spring onions, green and white parts separated and finely sliced, 6 garlic cloves, finely chopped, 1 tbsp. grated ginger, small bunch of basil, or purple basil, leaves picked and roughly chopped, cooked rice (about 250g uncooked weight), or cooked rice noodles, 50g roasted peanuts, roughly chopped\"]\n",
    "test_input = ['6 slices bacon, chopped, 0.5 c. chopped onion, <mask> c. chopped celery, 0.5 c. chopped green bell pepper, 3 cloves garlic, minced, 0.5 tsp. kosher salt, 0.5 tsp. freshly ground black pepper, 0.5 tsp. dried thyme, 0.25 tsp. cayenne pepper, 2 (12 ounce) packages frozen black-eyed peas, 4 c. reduced-sodium chicken broth, 2 c. cooked long grain rice, 0.25 c. chopped green onions, or as needed']\n",
    "# test_input = ['<mask> lb. lean beef, cut across the grain into thin slices (you need a quick-cooking cut, such as minute steak), 1 lemongrass stalk, trimmed and finely chopped, <mask> tbsp. soy sauce, 2 tbsp. fish sauce, 4 tsp. brown sugar, 0.5 tsp. chili flakes, 1 lime, juiced, 3 tbsp. vegetable oil, 1 green pepper, thinly sliced, 2 bunches of spring onions, green and white parts separated and finely sliced, 6 garlic cloves, finely chopped, 1 tbsp. grated ginger, small bunch of basil, or purple basil, leaves picked and roughly chopped, cooked rice (about 250g uncooked weight), or cooked rice noodles, 50g roasted peanuts, roughly chopped']\n",
    "test_input = [\"0.25 c. unsalted butter, 0.25 c. all-purpose flour, 1 medium onion, chopped, 1 c. chopped celery, 0.67 c. chopped red bell pepper, 0.67 c. chopped green bell pepper, 2 cloves garlic, minced, 2 tsp. Cajun seasoning, 0.5 c. chicken broth, 0.5 lb. thinly sliced, fully cooked smoked sausage, 0.25 tsp. crushed red pepper flakes, 0.5 tsp. freshly ground black pepper, salt to taste (see Cook's Notes), 0.5 lb. raw shrimp, peeled and deveined (see Cook's Notes), 0.5 c. heavy whipping cream, <mask> oz. spaghetti, cooked, chopped fresh parsley for garnish (optional), lemon slices for garnish (optional)\"]\n",
    "def test_pipeline(test_input):\n",
    "    single_df = pd.DataFrame({\"ingr\":test_input, \"tags\":[0.0]})\n",
    "    # single_df\n",
    "    s_test_customset = CustomDataset(single_df, tokenizer, MAX_LEN)\n",
    "    s_test_loader = DataLoader(s_test_customset, batch_size=1, shuffle=False, collate_fn=colbatch)\n",
    "    with torch.no_grad():\n",
    "        for _, (id, mask, tokentype, _) in enumerate(s_test_loader):\n",
    "            outputs = model_best(id, mask, tokentype)\n",
    "            print(outputs.logits.tolist()[0][0])\n",
    "            print(scaler.inverse_transform(outputs.logits.tolist())[0][0])\n",
    "            return(outputs.logits.tolist()[0][0])\n",
    "            \n",
    "            # return outputs.logits.tolist(), scaler.inverse_transform(outputs.logits.tolist())\n",
    "test_pipeline(test_input)\n",
    "scaler.transform([[12]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
